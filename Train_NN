#from sklearn.model_selection import train_test_split
import cv2
import pandas as pd
import numpy as np
import tensorflow
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.python.keras.layers import Conv2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Dense
from tensorflow.python.keras.models import Sequential
#import keras
#from tensorflow.keras import Sequential
from tensorflow.keras.optimizers import Adam
#from keras.layers import Conv2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Dense
import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os


# Die Bilder sollen geladen werden 
data = pd.read_csv("path_to_csv/data.csv", sep=',') #sep = ';' falls mit ; getrennt

def load_img_steering(datadir, data):
  image_path = []
  steering = []
  throttle = []
  brake = []
  for i in range(len(data)):
    indexed_data = data.iloc[i] #indexed_data ist nur eine Zeile  der ganzen Tabelle
    image_path.append(os.path.join(datadir, str(int(indexed_data[0]))+'.jpg'))
    #image_path.append(indexed_data[0]) #
    brake.append(float(indexed_data[1]))
    throttle.append(float(indexed_data[2]))
    steering.append(float(indexed_data[3]))

  image_paths = np.asarray(image_path)
  steerings = np.asarray(steering)
  throttle = np.asarray(throttle)
  return image_paths, steerings, throttle
 
image_paths, steerings, throttle = load_img_steering('path_to_pictures', data)


# Train-Test-Split durchführen 
#X_train, X_valid, y_train, y_valid = train_test_split(image_paths, steerings, test_size=0.2, random_state=6)
#print('Training Samples: {}\nValid Samples: {}'.format(len(X_train), len(X_valid)))

def img_preprocess(img):
    #img = img[60:135,:,:] #Das Bild kann zugeschnitten werden auf einen bestimmten Bereich
  img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV) #colormap für NVIDIA-Model
    #img = cv2.GaussianBlur(img,  (3, 3), 0) #Blur um Bild zu smoothen
  img = cv2.resize(img, (200, 66)) #Auf richtige Größe für Model bringen
  img = img/255 #Bild normalisieren
  return img

def batch_generator(image_paths, steering_ang, throttle_list, batch_size, istraining):
  
  while True:
    batch_img = []
    batch_steering = []
    batch_throttle = []
    for i in range(batch_size):

      random_index = random.randint(0, len(image_paths) - 1)

      if istraining: #falls die Funktion im Modelltraining aufgerufen werden soll, dann sollen die Bilder augmentiert werden
        im = mpimg.imread(image_paths[random_index])
        steering = steering_ang[random_index]  
        throttle = throttle_list[random_index]
      else:
        print('die Einstellung für istraining ist falsch gewählt"')

      im = img_preprocess(im) 
      batch_img.append(im)
      batch_steering.append(steering)
      batch_throttle.append(throttle)

    yield (np.asarray(batch_img), np.asarray(batch_steering), np.asarray(batch_throttle))



def nvidia_model():
  model = Sequential()
  model.add(Conv2D(24, kernel_size=(5,5), strides=(2,2), input_shape=(66,200,3),activation='relu'))
  
  model.add(Conv2D(36, kernel_size=(5,5), strides=(2,2), activation='relu'))
  model.add(Conv2D(48, kernel_size=(5,5), strides=(2,2), activation='relu'))
  model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
  model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
  model.add(Dropout(0.5))
  
  
  model.add(Flatten()) # Mit Flatten wird aus den feature maps eine aneinanderreihung von Knoten erstellt, die im nächsten fully-connected layer verarbeitet werden
  
  model.add(Dense(100, activation = 'elu')) #elu ist kein Schreibfehler
#   model.add(Dropout(0.5))
  
  model.add(Dense(50, activation = 'elu')) #elu verhindert ein dead-relu
#   model.add(Dropout(0.5))
  
  model.add(Dense(10, activation = 'elu'))
#   model.add(Dropout(0.5))
 
  model.add(Dense(2)) # Der letzte Layer hat zwei Knoten, einmal für steering_angle und einmal für Gas/Bremse
  
  optimizer = Adam(lr=1e-3)
  model.compile(loss='mse', optimizer=optimizer) #Mean Squared Error
  return model

model = nvidia_model()
print(model.summary())


# Hier könnte man ein earyl-stopping integrieren, dass das Training abgebrochen wird, wenn die Validation_loss wieder steigt
early_stopping_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1)
#history = model.fit(X_train, y_train,
 #                                 epochs=20,
  #                                validation_data=X_valid, y_valid,
   #                               callbacks=[early_stopping_callback])
    #                              verbose=1,
     #                             shuffle = 1)
history = model.fit_generator(batch_generator(image_paths, steerings, throttle, 100, 1),
                                  steps_per_epoch=300, 
                                  epochs=10,
                                  verbose=1,
                                  shuffle = 1)
#history = model.fit_generator(batch_generator(image_paths, steerings, throttle, 100, 1),
 #                                 steps_per_epoch=300, 
  #                                epochs=10,
 #                                validation_data=batch_generator(X_valid, y_valid, 100, 0),
  #                                validation_steps=200,
   #                               verbose=1,
    #                              shuffle = 1)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training', 'validation'])
plt.title('Loss')
plt.xlabel('Epoch')

model.save('model.h5') #Hier kann das Modell gespeichert werden, um es später zu verwenden
